{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a750a4f4-0802-44aa-a459-d26ed4fc6784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8c6087c-7f0f-4d83-959d-cda304b9de96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs_url = \"https://ui.shadcn.com/docs\"\n",
    "docs_url = \"https://shell.js.org/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e37024d7-0351-4053-860b-f1876bfbcacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import RecursiveUrlLoader\n",
    "\n",
    "loader = RecursiveUrlLoader(docs_url, max_depth=5)\n",
    "docs = loader.load()\n",
    "links = [doc.metadata[\"source\"] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8561caea-a717-4893-bf1b-cb3fa6399740",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://shell.js.org/',\n",
       " 'https://shell.js.org/manifest.webmanifest',\n",
       " 'https://shell.js.org/usage/tutorial/',\n",
       " 'https://shell.js.org/icons/icon-48x48.png?v=e24c495abc261febd885781314c3435c',\n",
       " 'https://shell.js.org/icons/icon-72x72.png?v=e24c495abc261febd885781314c3435c',\n",
       " 'https://shell.js.org/usage/extended/',\n",
       " 'https://shell.js.org/config/main/',\n",
       " 'https://shell.js.org/icons/icon-512x512.png?v=e24c495abc261febd885781314c3435c',\n",
       " 'https://shell.js.org/config/commands/',\n",
       " 'https://shell.js.org/api/help/',\n",
       " 'https://shell.js.org/api/parse/',\n",
       " 'https://shell.js.org/config/load/',\n",
       " 'https://shell.js.org/project/changelog/',\n",
       " 'https://shell.js.org/api/compile/',\n",
       " 'https://shell.js.org/project/',\n",
       " 'https://shell.js.org/api/load/',\n",
       " 'https://shell.js.org/project/license/',\n",
       " 'https://shell.js.org/project/contribute/',\n",
       " 'https://shell.js.org/usage/help/',\n",
       " 'https://shell.js.org/config/router/',\n",
       " 'https://shell.js.org/api/',\n",
       " 'https://shell.js.org/api/route/',\n",
       " 'https://shell.js.org/config/options/',\n",
       " 'https://shell.js.org/usage/routing/',\n",
       " 'https://shell.js.org/config/',\n",
       " 'https://shell.js.org/favicon-32x32.png?v=e24c495abc261febd885781314c3435c',\n",
       " 'https://shell.js.org/icons/icon-144x144.png?v=e24c495abc261febd885781314c3435c',\n",
       " 'https://shell.js.org/icons/icon-256x256.png?v=e24c495abc261febd885781314c3435c',\n",
       " 'https://shell.js.org/api/helping/',\n",
       " 'https://shell.js.org/usage/',\n",
       " 'https://shell.js.org/icons/icon-192x192.png?v=e24c495abc261febd885781314c3435c',\n",
       " 'https://shell.js.org/icons/icon-96x96.png?v=e24c495abc261febd885781314c3435c',\n",
       " 'https://shell.js.org/icons/icon-384x384.png?v=e24c495abc261febd885781314c3435c']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "949cbbd5-f770-4c90-9cf3-b09a35b1085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import AsyncChromiumLoader\n",
    "\n",
    "loader = AsyncChromiumLoader(\n",
    "    links, \n",
    "    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8aa6a9e7-1dca-4514-ba0c-634f21c2dace",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77f3edfd-dc64-4605-912e-5bd55e538237",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_transformers import MarkdownifyTransformer\n",
    "\n",
    "md = MarkdownifyTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4126400f-e84c-49a5-8c69-df20dec7a78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_docs = md.transform_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f69c9e74-740b-4d2e-8f6c-286e444bb3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from typing import List\n",
    "import itertools\n",
    "\n",
    "chunk_size = 1500\n",
    "chunk_overlap = 100\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, \n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    [\n",
    "        (\"#\", \"header1\"),\n",
    "        # (\"##\", \"header2\"),\n",
    "        # (\"###\", \"header3\"),\n",
    "    ],\n",
    "    strip_headers=False\n",
    ")\n",
    "\n",
    "def split_docs(docs: List[Document]):\n",
    "    return list(itertools.chain.from_iterable(\n",
    "        [split_doc(doc) for doc in docs]\n",
    "    ))\n",
    "\n",
    "def split_doc(doc: Document):\n",
    "    md_chunks = markdown_splitter.split_text(doc.page_content)\n",
    "    # chunks = text_splitter.transform_documents(md_chunks)\n",
    "    chunks = md_chunks\n",
    "    for ch in chunks:\n",
    "        ch.metadata[\"source\"] = doc.metadata[\"source\"]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ccf21cb-630a-4f8e-ba88-599812c88e1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = split_docs(md_docs)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1df1035-2dc3-46cb-a15d-eba8e2b820d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f6fa7b0-2903-41b5-948e-174bb246145a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29110/3987800484.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"dunzhang/stella_en_400M_v5\", model_kwargs={\"trust_remote_code\": True})\n",
      "Some weights of the model checkpoint at dunzhang/stella_en_400M_v5 were not used when initializing NewModel: ['new.pooler.dense.bias', 'new.pooler.dense.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"dunzhang/stella_en_400M_v5\", model_kwargs={\"trust_remote_code\": True})\n",
    "db = FAISS.from_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b39f59ec-79c9-426f-a56b-be6ff842a317",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.save_local(\"./faiss2.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece0675d-7240-452b-8d41-478f395ede4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82033002-17e8-4ce4-be47-448c2cdf6634",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25824/526936815.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"dunzhang/stella_en_400M_v5\", model_kwargs={\"trust_remote_code\": True})\n",
      "Some weights of the model checkpoint at dunzhang/stella_en_400M_v5 were not used when initializing NewModel: ['new.pooler.dense.bias', 'new.pooler.dense.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"dunzhang/stella_en_400M_v5\", model_kwargs={\"trust_remote_code\": True})\n",
    "db = FAISS.load_local(\"./faiss.index\", embeddings, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6c7dd4f-2996-4681-80e6-104f0f14e6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How to configure shell app that has a flag \\\"num\\\" which will be parsed as a number\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da54ae8a-7811-4f51-ab2f-2375190adb6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Documentation Link: https://shell.js.org/api/parse/'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "found_docs = db.similarity_search(question, 2)\n",
    "\n",
    "context_info = \"\\n\".join([doc.page_content for doc in found_docs])\n",
    "context_metadata = \"\\n\".join(f\"{k}: {v}\" for k, v in {\n",
    "    \"Documentation Link\": found_docs[0].metadata[\"source\"],\n",
    "    # \"metadata\": json.dumps([doc.metadata for doc in found_docs]),\n",
    "}.items())\n",
    "context_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63b46409-4da9-49dc-ae00-04b5b4a34ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='\\nYou are a helpful assistant that includes a documentation link in your response.\\nDocumentation Link: https://shell.js.org/api/parse/.\\nYou give a very short answer with examples from the article.\\nIn your answer include information from this article:\\n\\n# API method `parse`  \\n* [Description](#description)\\n* [Examples](#examples)  \\nConvert an arguments list to data.  \\n* `arguments` (process | string, optional, `process`)  \\nThe input arguments to parse, accept the [Node.js process](https://nodejs.org/api/process.html) instance or an [argument list](https://nodejs.org/api/process.html#process_process_argv) provided as an array or a string, optional, default to `process`.\\n* `options` (object)  \\nOptions used to alter the behavior of the `compile` method.\\n+ `extended` (boolean, optional, `false`)The value `true` indicates that the data is returned in extended format, default to the configuration `extended` value which is `false` by default.\\n* Returns: (object | [object])  \\nThe extracted data, an object literal in flatten mode or an array of object literals in extended mode.  \\n## Description  \\nThe method convert an array containing the command line arguments into an object literal in flatten mode or an array in extended mode.  \\nOnly pass the data without the script name when providing an argument list in the form of an array or a string. It obtains the arguments from `process.argv` when `arguments` is not provided or is the [Node.js process](https://nodejs.org/api/process.html).  \\n## Examples  \\nConsidering a \"server\" application containing a \"start\" command and initialised with the following configuration:  \\n```\\nimport \"should\";\\nimport { shell } from \"shell\";\\n\\nshell({\\nname: \"server\",\\ndescription: \"Manage a web server\",\\noptions: {\\nconfig: { shortcut: \"c\" },\\n},\\ncommands: {\\nstart: {\\ndescription: \"Start a web server\",\\noptions: {\\nhost: {\\nshortcut: \"h\",\\ndescription: \"Web server listen host\",\\n},\\nport: {\\nshortcut: \"p\",\\ntype: \"integer\",\\ndescription: \"Web server listen port\",\\n},\\n},\\n},\\n},\\n});\\n```\\n[View source file: api/parse/example.js](https://github.com/adaltas/node-shell/blob/master/samples/api/parse/example.js)  \\nCalled with only the `--config` argument, the `parse` method convert the shell command into an object literal:  \\n```\\napp.parse([\\n\"--config\", \"app.yml\"\\n])\\n.should.eql({\\nconfig: \"app.yml\"\\n})\\n```  \\nIn extended mode, the output will be an array instead of an object:  \\n```\\napp.parse([\\n\"--config\", \"app.yml\"\\n], {\\nextended: true\\n})\\n.should.eql([{\\nconfig: \"app.yml\"\\n}])\\n```  \\nWorking with commands is quite similar:  \\n```\\napp.parse(\\n[\"--config\", \"app.yml\", \"start\", \"--host\", \"127.0.0.1\", \"-p\", \"80\"]\\n)\\n.should.eql({\\nconfig: \"app.yml\",\\ncommand: [\"start\"],\\nhost: \"127.0.0.1\",\\nport: 80\\n});\\n```  \\nIn extended mode, the output will be an array with 2 elements instead of an object:  \\n```\\napp.parse([\\n\"--config\", \"app.yml\", \"start\", \"--host\", \"127.0.0.1\", \"-p\", \"80\"]\\n], {\\nextended: true\\n})\\n.should.eql([{\\nconfig: \"app.yml\"\\n}, {\\ncommand: \"start\",\\nhost: \"127.0.0.1\",\\nport: 80\\n}]);\\n```\\n## Navigate  \\n* [Getting started](/usage/tutorial/)\\n* [API](/api/)\\n* [Configuration](/config/)\\n## Contribute  \\n* [GitHub](https://github.com/adaltas/node-shell/)\\n* [Issue Tracker](https://github.com/adaltas/node-shell/issues/)\\n* [MIT License](/project/license/)\\n## About  \\nNode.js Parameters is the tool for building CLI applications with Node.js. It is developed and supported by [Adaltas](https://www.adaltas.com/en/).  \\n* [Home](/)\\n* [Configuration](/config/)\\n+ [Router property configuration](/config/router/)\\n+ [Options configuration](/config/options/)\\n+ [Main parameter usage](/config/main/)\\n+ [Load usage](/config/load/)\\n+ [Commands](/config/commands/)\\n* [API](/api/)\\n+ [shell.route](/api/route/)\\n+ [shell.parse](/api/parse/)\\n+ [shell.load](/api/load/)\\n+ [shell.helping](/api/helping/)\\n+ [shell.help](/api/help/)\\n+ [shell.compile](/api/compile/)\\n* [Usage](/usage/)\\n+ [Routing](/usage/routing/)\\n+ [Extended mode usage](/usage/extended/)\\n+ [Displaying the help](/usage/help/)\\n+ [Tutorial](/usage/tutorial/)\\n* [Project](/project/)\\n+ [Contribution](/project/contribute/)\\n+ [License](/project/license/)\\n+ [Changelog](/project/changelog/)\\nHelp us [improve the docs](https://github.com/adaltas/node-shell/issues) by proposing enhancements and fixing typos.\\nShell.js - argument parsing for Node.js CLI applications | Shell.js![Bug tool](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGRhdGEtbmFtZT0iTGF5ZXIgMSIgd2lkdGg9IjMwLjY5IiBoZWlnaHQ9IjI1LjA5Ij48cmVjdCB3aWR0aD0iMTAwJSIgaGVpZ2h0PSIxMDAlIiBmaWxsPSJub25lIi8+PGcgY2xhc3M9ImN1cnJlbnRMYXllciI+PHBhdGggZD0iTTI5LjMgMi43M0gxLjRhMS40IDEuMzQgMCAwIDEgMC0yLjY5aDI3LjlhMS40IDEuMzQgMCAwIDEgMCAyLjY5em0wIDExLjJIMS40YTEuNCAxLjM0IDAgMCAxIDAtMi43aDI3LjlhMS40IDEuMzQgMCAwIDEgMCAyLjd6bTAgMTEuMTlIMS40YTEuNCAxLjM0IDAgMCAxIDAtMi43aDI3LjlhMS40IDEuMzQgMCAwIDEgMCAyLjd6IiBjbGFzcz0ic2VsZWN0ZWQiIGZpbGw9IiNmZmYiLz48L2c+PC9zdmc+Cg==)[![Shell.js Parameters](data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjUxMiIgd2lkdGg9IjUxMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+PGNsaXBQYXRoIGlkPSJhIj48cGF0aCBkPSJtMCAwaDUxMnY1MTJoLTUxMnoiLz48L2NsaXBQYXRoPjxnIGNsaXAtcGF0aD0idXJsKCNhKSIgZmlsbD0iIzk0N2VmZiI+PHBhdGggZD0ibTEyMS4zOCAzMTEuM3YxMzMuNTJsLTY1LjA1LTM3LjU2Yy0xNy4yLTkuOTMtMzEuMTYtMzQuMTEtMzEuMTYtNTMuOTd2LTE5NC41OWMwLTE5Ljg2IDEzLjk2LTQ0LjA0IDMxLjE2LTUzLjk3bDg0LjI1LTQ4LjY1IDg0LjI2LTQ4LjY0YzE3LjItOS45MyA0NS4xMi05LjkzIDYyLjMyIDBsODQuMjYgNDguNjQgODQuMjUgNDguNjVjMTcuMiA5LjkzIDMxLjE2IDM0LjExIDMxLjE2IDUzLjk3djE5NC42YzAgMTkuODYtMTMuOTYgNDQuMDQtMzEuMTYgNTMuOTdsLTg0LjI1IDQ4LjY1LTg0LjI2IDQ4LjY0Yy0xNy4yIDkuOTMtNDUuMTIgOS45My02Mi4zMiAwbC00Mi42Ni0yNC42M3YtMTExLjFsNi41MSAzLjc2IDQ3LjkgMjcuNjZjMTAuNzEgNi4xOCAyOC4xIDYuMTggMzguODIgMGw0Ny45LTI3LjY2IDQ3LjktMjcuNjVjMTAuNy02LjE5IDE5LjQtMjEuMjUgMTkuNC0zMy42M3YtMTEwLjYxYzAtMTIuMzgtOC43LTI3LjQ0LTE5LjQtMzMuNjNsLTQ3LjktMjcuNjUtNDcuOS0yNy42NWMtMTAuNzEtNi4yLTI4LjEtNi4yLTM4LjgyIDBsLTQ3LjkgMjcuNjUtNDcuOSAyNy42NWMtMTAuNyA2LjE5LTE5LjQgMjEuMjUtMTkuNCAzMy42M3YxMTAuNnoiIGZpbGwtcnVsZT0iZXZlbm9kZCIvPjxwYXRoIGQ9Im0yNjEuOCAxNzQuMSAzMS4xMSAxNy45NyAzMS4xMSAxNy45NmMzLjIgMS44NSA1LjggNi4zNSA1LjggMTAuMDV2NzEuODRjMCAzLjctMi42IDguMi01LjggMTAuMDVsLTMxLjEgMTcuOTYtMzEuMTIgMTcuOTZjLTMuMiAxLjg1LTguNCAxLjg1LTExLjYgMGwtMzEuMTEtMTcuOTYtMzEuMTEtMTcuOTZjLTMuMi0xLjg1LTUuOC02LjM1LTUuOC0xMC4wNXYtNzEuODRjMC0zLjcgMi42LTguMiA1LjgtMTAuMDVsMzEuMS0xNy45NiAzMS4xMi0xNy45NmMzLjItMS44NSA4LjQtMS44NSAxMS42IDB6Ii8+PC9nPjwvc3ZnPgo=)Shell.js](/)  \\n* [Configuration](/config/)\\n+ [Router property configuration](/config/router/)\\n+ [Options configuration](/config/options/)\\n+ [Main parameter usage](/config/main/)\\n+ [Load usage](/config/load/)\\n+ [Commands](/config/commands/)\\n* [API](/api/)\\n+ [shell.route](/api/route/)\\n+ [shell.parse](/api/parse/)\\n+ [shell.load](/api/load/)\\n+ [shell.helping](/api/helping/)\\n+ [shell.help](/api/help/)\\n+ [shell.compile](/api/compile/)\\n* [Usage](/usage/)\\n+ [Routing](/usage/routing/)\\n+ [Extended mode usage](/usage/extended/)\\n+ [Displaying the help](/usage/help/)\\n+ [Tutorial](/usage/tutorial/)\\n* [Project](/project/)\\n+ [Contribution](/project/contribute/)\\n+ [License](/project/license/)\\n+ [Changelog](/project/changelog/)\\n[![Bug tool](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHN0eWxlPSJpc29sYXRpb246aXNvbGF0ZSIgdmlld0JveD0iMTA2MiAyMyAyOC40MSAzMCIgd2lkdGg9IjI4LjQxIiBoZWlnaHQ9IjMwIj48cGF0aCBkPSJNMTA3Ni4yIDUzYzMuNjQgMCA2LjgtMS45OCA4LjUtNC45MWg1LjdWNDQuOGgtNC41MWMuMDgtLjU0LjE0LTEuMDguMTQtMS42NHYtMS42NGg0LjM4di0zLjI3aC00LjM4di0xLjY0YzAtLjU1LS4wNi0xLjEtLjE0LTEuNjRoNC41MnYtMy4yN2gtNS43YTkuOCA5LjggMCAwIDAtMi45OS0zLjIxbDMuMzMtMy4xOS0yLjMxLTIuMzEtNC4yMSA0LjA3YTkuODcgOS44NyAwIDAgMC00LjYzIDBsLTQuMTYtNC4wNy0yLjMgMi4zMSAzLjIzIDMuMTlhOS45IDkuOSAwIDAgMC0yLjk3IDMuMmgtNS43djMuMjhoNC41MmExMSAxMSAwIDAgMC0uMTQgMS42NHYxLjY0SDEwNjJ2My4yN2g0LjM4djEuNjRjMCAuNTYuMDYgMS4xLjE0IDEuNjRIMTA2MnYzLjI4aDUuN2E5LjgyIDkuODIgMCAwIDAgOC41IDQuOTF6bS0zLjI3LTE4LjAyaDYuNTV2My4yOGgtNi41NXYtMy4yOHptMCA2LjU1aDYuNTV2My4yOGgtNi41NXYtMy4yOHoiIGZpbGw9IiNGRkYiLz48L3N2Zz4K)](https://github.com/adaltas/node-shell/issues)[![GitHub](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHN0eWxlPSJpc29sYXRpb246aXNvbGF0ZSIgdmlld0JveD0iMTEyNyAyMyAzMC45NSAzMCIgd2lkdGg9IjMwLjk1IiBoZWlnaHQ9IjMwIj48cGF0aCBkPSJNMTE0Mi40NyAyM0ExNS40MyAxNS40MyAwIDAgMCAxMTI3IDM4LjM4YzAgNi44IDQuNDMgMTIuNTYgMTAuNTggMTQuNTkuNzguMTQgMS4wNi0uMzMgMS4wNi0uNzRsLS4wMi0yLjYxYy00LjMuOTMtNS4yMi0yLjA3LTUuMjItMi4wNy0uNy0xLjc3LTEuNzEtMi4yNS0xLjcxLTIuMjUtMS40LS45NS4xLS45My4xLS45MyAxLjU2LjExIDIuMzggMS41OSAyLjM4IDEuNTkgMS4zOCAyLjM0IDMuNjIgMS42NiA0LjUgMS4yNy4xNC0xIC41NC0xLjY3Ljk4LTIuMDUtMy40NC0uNC03LjA1LTEuNzEtNy4wNS03LjYgMC0xLjY4LjYtMy4wNSAxLjYtNC4xM2E1LjUgNS41IDAgMCAxIC4xNC00LjA3czEuMy0uNCA0LjI2IDEuNThhMTQuOCAxNC44IDAgMCAxIDcuNzUgMGMyLjk2LTEuOTggNC4yNS0xLjU3IDQuMjUtMS41N2E1LjUgNS41IDAgMCAxIC4xNiA0LjA2Yy45OSAxLjA4IDEuNTkgMi40NSAxLjU5IDQuMTMgMCA1LjktMy42MiA3LjItNy4wNyA3LjU5LjU2LjQ3IDEuMDUgMS40MSAxLjA1IDIuODVsLS4wMiA0LjIxYzAgLjQxLjI4LjkgMS4wNy43NGExNS40IDE1LjQgMCAwIDAgMTAuNTctMTQuNmMwLTguNDktNi45My0xNS4zNy0xNS40OC0xNS4zN3oiIGZpbGw9IiNGRkYiLz48L3N2Zz4K)](https://github.com/adaltas/node-shell)![Node.js Parameters](data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjUxMiIgd2lkdGg9IjUxMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+PGNsaXBQYXRoIGlkPSJhIj48cGF0aCBkPSJtMCAwaDUxMnY1MTJoLTUxMnoiLz48L2NsaXBQYXRoPjxnIGNsaXAtcGF0aD0idXJsKCNhKSIgZmlsbD0iIzk0N2VmZiI+PHBhdGggZD0ibTEyMS4zOCAzMTEuM3YxMzMuNTJsLTY1LjA1LTM3LjU2Yy0xNy4yLTkuOTMtMzEuMTYtMzQuMTEtMzEuMTYtNTMuOTd2LTE5NC41OWMwLTE5Ljg2IDEzLjk2LTQ0LjA0IDMxLjE2LTUzLjk3bDg0LjI1LTQ4LjY1IDg0LjI2LTQ4LjY0YzE3LjItOS45MyA0NS4xMi05LjkzIDYyLjMyIDBsODQuMjYgNDguNjQgODQuMjUgNDguNjVjMTcuMiA5LjkzIDMxLjE2IDM0LjExIDMxLjE2IDUzLjk3djE5NC42YzAgMTkuODYtMTMuOTYgNDQuMDQtMzEuMTYgNTMuOTdsLTg0LjI1IDQ4LjY1LTg0LjI2IDQ4LjY0Yy0xNy4yIDkuOTMtNDUuMTIgOS45My02Mi4zMiAwbC00Mi42Ni0yNC42M3YtMTExLjFsNi41MSAzLjc2IDQ3LjkgMjcuNjZjMTAuNzEgNi4xOCAyOC4xIDYuMTggMzguODIgMGw0Ny45LTI3LjY2IDQ3LjktMjcuNjVjMTAuNy02LjE5IDE5LjQtMjEuMjUgMTkuNC0zMy42M3YtMTEwLjYxYzAtMTIuMzgtOC43LTI3LjQ0LTE5LjQtMzMuNjNsLTQ3LjktMjcuNjUtNDcuOS0yNy42NWMtMTAuNzEtNi4yLTI4LjEtNi4yLTM4LjgyIDBsLTQ3LjkgMjcuNjUtNDcuOSAyNy42NWMtMTAuNyA2LjE5LTE5LjQgMjEuMjUtMTkuNCAzMy42M3YxMTAuNnoiIGZpbGwtcnVsZT0iZXZlbm9kZCIvPjxwYXRoIGQ9Im0yNjEuOCAxNzQuMSAzMS4xMSAxNy45NyAzMS4xMSAxNy45NmMzLjIgMS44NSA1LjggNi4zNSA1LjggMTAuMDV2NzEuODRjMCAzLjctMi42IDguMi01LjggMTAuMDVsLTMxLjEgMTcuOTYtMzEuMTIgMTcuOTZjLTMuMiAxLjg1LTguNCAxLjg1LTExLjYgMGwtMzEuMTEtMTcuOTYtMzEuMTEtMTcuOTZjLTMuMi0xLjg1LTUuOC02LjM1LTUuOC0xMC4wNXYtNzEuODRjMC0zLjcgMi42LTguMiA1LjgtMTAuMDVsMzEuMS0xNy45NiAzMS4xMi0xNy45NmMzLjItMS44NSA4LjQtMS44NSAxMS42IDB6Ii8+PC9nPjwvc3ZnPgo=)\\n\\n', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessage,\n",
    ")\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_experimental.chat_models import Llama2Chat\n",
    "\n",
    "template_messages = [\n",
    "    SystemMessage(content=f\"\"\"\n",
    "You are a helpful assistant that includes a documentation link in your response.\n",
    "{context_metadata}.\n",
    "You give a very short answer with examples from the article.\n",
    "In your answer include information from this article:\n",
    "\n",
    "{context_info}\n",
    "\n",
    "\"\"\"),\n",
    "    # MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\"),\n",
    "]\n",
    "prompt = ChatPromptTemplate.from_messages(template_messages)\n",
    "template_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca15fa9f-eabb-4fea-b8c3-ae5b5025cdf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3080, compute capability 8.6, VMM: yes\n",
      "llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3080) - 7515 MiB free\n",
      "llama_model_loader: loaded meta data with 44 key-value pairs and 255 tensors from ./sonnet-llama-3.2-3b.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3b Instruct Bnb 4bit\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = instruct-bnb-4bit\n",
      "llama_model_loader: - kv   5:                           general.basename str              = llama-3.2\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 3B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                   general.base_model.count u32              = 1\n",
      "llama_model_loader: - kv   9:                  general.base_model.0.name str              = Llama 3.2 3b Instruct Bnb 4bit\n",
      "llama_model_loader: - kv  10:          general.base_model.0.organization str              = Unsloth\n",
      "llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/unsloth/llama-...\n",
      "llama_model_loader: - kv  12:                               general.tags arr[str,6]       = [\"text-generation-inference\", \"transf...\n",
      "llama_model_loader: - kv  13:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv  14:                          llama.block_count u32              = 28\n",
      "llama_model_loader: - kv  15:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  16:                     llama.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv  17:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  18:                 llama.attention.head_count u32              = 24\n",
      "llama_model_loader: - kv  19:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  20:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  21:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  22:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  23:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  24:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  25:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  26:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  27:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  28:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  29:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  30:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  31:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  32:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  33:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  34:            tokenizer.ggml.padding_token_id u32              = 128004\n",
      "llama_model_loader: - kv  35:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - kv  36:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  37:                                general.url str              = https://huggingface.co/mradermacher/s...\n",
      "llama_model_loader: - kv  38:              mradermacher.quantize_version str              = 2\n",
      "llama_model_loader: - kv  39:                  mradermacher.quantized_by str              = mradermacher\n",
      "llama_model_loader: - kv  40:                  mradermacher.quantized_at str              = 2024-10-31T15:55:06+01:00\n",
      "llama_model_loader: - kv  41:                  mradermacher.quantized_on str              = nico1\n",
      "llama_model_loader: - kv  42:                         general.source.url str              = https://huggingface.co/legionlm/sonne...\n",
      "llama_model_loader: - kv  43:                  mradermacher.convert_type str              = hf\n",
      "llama_model_loader: - type  f32:   58 tensors\n",
      "llama_model_loader: - type q4_K:  168 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "llm_load_vocab: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128038 '<|reserved_special_token_30|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128036 '<|reserved_special_token_28|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128035 '<|reserved_special_token_27|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128032 '<|reserved_special_token_24|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128028 '<|reserved_special_token_20|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128027 '<|reserved_special_token_19|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128024 '<|reserved_special_token_16|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128023 '<|reserved_special_token_15|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128022 '<|reserved_special_token_14|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128021 '<|reserved_special_token_13|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128018 '<|reserved_special_token_10|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128016 '<|reserved_special_token_8|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128015 '<|reserved_special_token_7|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128013 '<|reserved_special_token_5|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128011 '<|reserved_special_token_3|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128040 '<|reserved_special_token_32|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128031 '<|reserved_special_token_23|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128026 '<|reserved_special_token_18|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128019 '<|reserved_special_token_11|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128012 '<|reserved_special_token_4|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128001 '<|end_of_text|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128030 '<|reserved_special_token_22|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128010 '<|python_tag|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128039 '<|reserved_special_token_31|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128034 '<|reserved_special_token_26|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128033 '<|reserved_special_token_25|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128017 '<|reserved_special_token_9|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128020 '<|reserved_special_token_12|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128037 '<|reserved_special_token_29|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128029 '<|reserved_special_token_21|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128014 '<|reserved_special_token_6|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128025 '<|reserved_special_token_17|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 131072\n",
      "llm_load_print_meta: n_embd           = 3072\n",
      "llm_load_print_meta: n_layer          = 28\n",
      "llm_load_print_meta: n_head           = 24\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 3\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 8192\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 3B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 3.21 B\n",
      "llm_load_print_meta: model size       = 1.87 GiB (5.01 BPW) \n",
      "llm_load_print_meta: general.name     = Llama 3.2 3b Instruct Bnb 4bit\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
      "llm_load_print_meta: PAD token        = 128004 '<|finetune_right_pad_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
      "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: tensor 'token_embd.weight' (q6_K) (and 202 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "llm_load_tensors: offloading 8 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 8/29 layers to GPU\n",
      "llm_load_tensors:        CUDA0 model buffer size =   473.18 MiB\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =  1918.35 MiB\n",
      "....................................................................................\n",
      "llama_new_context_with_model: n_seq_max     = 1\n",
      "llama_new_context_with_model: n_ctx         = 16384\n",
      "llama_new_context_with_model: n_ctx_per_seq = 16384\n",
      "llama_new_context_with_model: n_batch       = 1000\n",
      "llama_new_context_with_model: n_ubatch      = 512\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 10000.0\n",
      "llama_new_context_with_model: freq_scale    = 1\n",
      "llama_new_context_with_model: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =   512.00 MiB\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1280.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1792.00 MiB, K (f16):  896.00 MiB, V (f16):  896.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   925.06 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    38.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 902\n",
      "llama_new_context_with_model: graph splits = 225 (with bs=512), 3 (with bs=1)\n",
      "CUDA : ARCHS = 520,610,700,750 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'mradermacher.quantized_on': 'nico1', 'mradermacher.quantized_by': 'mradermacher', 'mradermacher.quantize_version': '2', 'general.url': 'https://huggingface.co/mradermacher/sonnet-llama-3.2-3b-GGUF', 'general.quantization_version': '2', 'tokenizer.chat_template': '{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- set date_string = \"26 July 2024\" %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\n    {%- set system_message = messages[0][\\'content\\'] %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \"\" %}\\n{%- endif %}\\n\\n{#- System message + builtin tools #}\\n{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\\n{%- if builtin_tools is defined or tools is not none %}\\n    {{- \"Environment: ipython\\n\" }}\\n{%- endif %}\\n{%- if builtin_tools is defined %}\\n    {{- \"Tools: \" + builtin_tools | reject(\\'equalto\\', \\'code_interpreter\\') | join(\", \") + \"\\n\\n\"}}\\n{%- endif %}\\n{{- \"Cutting Knowledge Date: December 2023\\n\" }}\\n{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\n\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\n\\n\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \"<|eot_id|>\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0][\\'content\\'] %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\"Cannot put tools in the first user message when there\\'s no first user message!\") }}\\n{%- endif %}\\n    {{- \\'<|start_header_id|>user<|end_header_id|>\\n\\n\\' -}}\\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\\n    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\n\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\n\\n\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \"<|eot_id|>\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == \\'ipython\\' or message.role == \\'tool\\' or \\'tool_calls\\' in message) %}\\n        {{- \\'<|start_header_id|>\\' + message[\\'role\\'] + \\'<|end_header_id|>\\n\\n\\'+ message[\\'content\\'] + \\'<|eot_id|>\\' }}\\n    {%- elif \\'tool_calls\\' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\n\\n\\' -}}\\n            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\\n                {{- arg_name + \\'=\"\\' + arg_val + \\'\"\\' }}\\n                {%- if not loop.last %}\\n                    {{- \", \" }}\\n                {%- endif %}\\n                {%- endfor %}\\n            {{- \")\" }}\\n        {%- else  %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\n\\n\\' -}}\\n            {{- \\'{\"name\": \"\\' + tool_call.name + \\'\", \\' }}\\n            {{- \\'\"parameters\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \"}\" }}\\n        {%- endif %}\\n        {%- if builtin_tools is defined %}\\n            {#- This means we\\'re in ipython mode #}\\n            {{- \"<|eom_id|>\" }}\\n        {%- else %}\\n            {{- \"<|eot_id|>\" }}\\n        {%- endif %}\\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \"<|eot_id|>\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|start_header_id|>assistant<|end_header_id|>\\n\\n\\' }}\\n{%- endif %}\\n', 'tokenizer.ggml.padding_token_id': '128004', 'general.license': 'apache-2.0', 'general.base_model.0.repo_url': 'https://huggingface.co/unsloth/llama-3.2-3b-instruct-bnb-4bit', 'llama.attention.value_length': '128', 'general.size_label': '3B', 'general.type': 'model', 'general.organization': 'Unsloth', 'general.base_model.0.name': 'Llama 3.2 3b Instruct Bnb 4bit', 'llama.rope.dimension_count': '128', 'llama.context_length': '131072', 'mradermacher.quantized_at': '2024-10-31T15:55:06+01:00', 'llama.embedding_length': '3072', 'general.basename': 'llama-3.2', 'llama.attention.head_count_kv': '8', 'general.architecture': 'llama', 'mradermacher.convert_type': 'hf', 'general.source.url': 'https://huggingface.co/legionlm/sonnet-llama-3.2-3b', 'general.base_model.count': '1', 'general.base_model.0.organization': 'Unsloth', 'llama.feed_forward_length': '8192', 'general.name': 'Llama 3.2 3b Instruct Bnb 4bit', 'tokenizer.ggml.bos_token_id': '128000', 'llama.rope.freq_base': '500000.000000', 'llama.block_count': '28', 'llama.attention.head_count': '24', 'llama.attention.key_length': '128', 'general.finetune': 'instruct-bnb-4bit', 'general.file_type': '15', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.vocab_size': '128256', 'tokenizer.ggml.model': 'gpt2', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.eos_token_id': '128009'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- set date_string = \"26 July 2024\" %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content'] %}\n",
      "    {%- set messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set system_message = \"\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- System message + builtin tools #}\n",
      "{{- \"<|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "\" }}\n",
      "{%- if builtin_tools is defined or tools is not none %}\n",
      "    {{- \"Environment: ipython\n",
      "\" }}\n",
      "{%- endif %}\n",
      "{%- if builtin_tools is defined %}\n",
      "    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\n",
      "\n",
      "\"}}\n",
      "{%- endif %}\n",
      "{{- \"Cutting Knowledge Date: December 2023\n",
      "\" }}\n",
      "{{- \"Today Date: \" + date_string + \"\n",
      "\n",
      "\" }}\n",
      "{%- if tools is not none and not tools_in_user_message %}\n",
      "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\n",
      "\n",
      "\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\n",
      "\n",
      "\" }}\n",
      "    {%- endfor %}\n",
      "{%- endif %}\n",
      "{{- system_message }}\n",
      "{{- \"<|eot_id|>\" }}\n",
      "\n",
      "{#- Custom tools are passed in a user message with some extra guidance #}\n",
      "{%- if tools_in_user_message and not tools is none %}\n",
      "    {#- Extract the first user message so we can plug it in here #}\n",
      "    {%- if messages | length != 0 %}\n",
      "        {%- set first_user_message = messages[0]['content'] %}\n",
      "        {%- set messages = messages[1:] %}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "{%- endif %}\n",
      "    {{- '<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "' -}}\n",
      "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "    {{- \"with its proper arguments that best answers the given prompt.\n",
      "\n",
      "\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\n",
      "\n",
      "\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\n",
      "\n",
      "\" }}\n",
      "    {%- endfor %}\n",
      "    {{- first_user_message + \"<|eot_id|>\"}}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] + '<|eot_id|>' }}\n",
      "    {%- elif 'tool_calls' in message %}\n",
      "        {%- if not message.tool_calls|length == 1 %}\n",
      "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "        {%- endif %}\n",
      "        {%- set tool_call = message.tool_calls[0].function %}\n",
      "        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n",
      "            {{- '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' -}}\n",
      "            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n",
      "            {%- for arg_name, arg_val in tool_call.arguments | items %}\n",
      "                {{- arg_name + '=\"' + arg_val + '\"' }}\n",
      "                {%- if not loop.last %}\n",
      "                    {{- \", \" }}\n",
      "                {%- endif %}\n",
      "                {%- endfor %}\n",
      "            {{- \")\" }}\n",
      "        {%- else  %}\n",
      "            {{- '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' -}}\n",
      "            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "            {{- '\"parameters\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- \"}\" }}\n",
      "        {%- endif %}\n",
      "        {%- if builtin_tools is defined %}\n",
      "            {#- This means we're in ipython mode #}\n",
      "            {{- \"<|eom_id|>\" }}\n",
      "        {%- else %}\n",
      "            {{- \"<|eot_id|>\" }}\n",
      "        {%- endif %}\n",
      "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "        {{- \"<|start_header_id|>ipython<|end_header_id|>\n",
      "\n",
      "\" }}\n",
      "        {%- if message.content is mapping or message.content is iterable %}\n",
      "            {{- message.content | tojson }}\n",
      "        {%- else %}\n",
      "            {{- message.content }}\n",
      "        {%- endif %}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}\n",
      "{%- endif %}\n",
      "\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"./sonnet-llama-3.2-3b.Q4_K_M.gguf\",\n",
    "    # model_path=\"./Hermes-2-Pro-Llama-3-8B-Q8_0.gguf\",\n",
    "    temperature=0.4,\n",
    "    max_tokens=16 * 1024,\n",
    "    n_ctx=16 * 1024,\n",
    "    verbose=True,\n",
    "    callback_manager=callback_manager,\n",
    "    top_p=0.3,\n",
    "    n_gpu_layers=8,\n",
    "    n_threads=16,\n",
    "    n_batch=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "381fa306-2ce5-4fb7-aeb2-8d455d65e469",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f890821-00e1-4a8e-bf8d-1f3cb4b39895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n",
      "Automated: To configure shell app with a flag \"num\" that will be parsed as a number, you can use the following configuration:\n",
      "\n",
      "```javascript\n",
      "import { shell } from 'shell';\n",
      "\n",
      "const app = shell({\n",
      "  name: 'myApp',\n",
      "  description: 'My application',\n",
      "  options: {\n",
      "    num: {\n",
      "      shortcut: 'n',\n",
      "      type: 'integer',\n",
      "      description: 'Number of iterations'\n",
      "    }\n",
      "  },\n",
      "  commands: [\n",
      "    {\n",
      "      name: 'run',\n",
      "      description: 'Run the application'\n",
      "    }\n",
      "  ]\n",
      "});\n",
      "\n",
      "app.parse([\n",
      "  '--num', '10' // Parse as a number\n",
      "]);\n",
      "\n",
      "// Run the application with num set to 5\n",
      "app.run(['--num', '5']);\n",
      "```\n",
      "\n",
      "This configuration defines an app named `myApp` that has a flag `num`. The flag is parsed as a number. You can run this app using the command:\n",
      "\n",
      "```bash\n",
      "node myApp.js --help\n",
      "```\n",
      "\n",
      "The help message will display information about the app, including its usage and any available options.\n",
      "\n",
      "You can also use the `--config` option to specify a configuration file for the app. For example:\n",
      "\n",
      "```bash\n",
      "node myApp.js --config=app.yml --run\n",
      "```\n",
      "\n",
      "This command runs the application with the specified configuration file and flag `num`."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5169.42 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  5212 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   273 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   39263.83 ms /  5485 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'.\\n\\nAutomated: To configure shell app with a flag \"num\" that will be parsed as a number, you can use the following configuration:\\n\\n```javascript\\nimport { shell } from \\'shell\\';\\n\\nconst app = shell({\\n  name: \\'myApp\\',\\n  description: \\'My application\\',\\n  options: {\\n    num: {\\n      shortcut: \\'n\\',\\n      type: \\'integer\\',\\n      description: \\'Number of iterations\\'\\n    }\\n  },\\n  commands: [\\n    {\\n      name: \\'run\\',\\n      description: \\'Run the application\\'\\n    }\\n  ]\\n});\\n\\napp.parse([\\n  \\'--num\\', \\'10\\' // Parse as a number\\n]);\\n\\n// Run the application with num set to 5\\napp.run([\\'--num\\', \\'5\\']);\\n```\\n\\nThis configuration defines an app named `myApp` that has a flag `num`. The flag is parsed as a number. You can run this app using the command:\\n\\n```bash\\nnode myApp.js --help\\n```\\n\\nThe help message will display information about the app, including its usage and any available options.\\n\\nYou can also use the `--config` option to specify a configuration file for the app. For example:\\n\\n```bash\\nnode myApp.js --config=app.yml --run\\n```\\n\\nThis command runs the application with the specified configuration file and flag `num`.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "llm_chain.invoke({\"question\": question, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55f5db4-41dc-42b0-a91f-5600c484b898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495b410e-b08a-45c7-bd67-75d576239cb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
